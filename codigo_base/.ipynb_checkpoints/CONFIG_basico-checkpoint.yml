---
# ============================================================================
# CONFIGURACI칍N PARA DESAF칈O DE HEALTH ECONOMICS
# Universidad del Oeste (UNO) - Aplicaciones en Ciencia de Datos
# ============================================================================
# Este archivo YAML define todos los par치metros para el pipeline de Machine Learning
# que predice el gasto de bolsillo en salud (hf3_ppp_pc) para el a침o 2022.
#
# El pipeline tiene 3 etapas:
#   1. Feature Engineering (01_FE): Creaci칩n de variables predictoras
#   2. Training Strategy (02_TS): Particionamiento de datos en train/validate/test
#   3. Hyperparameter Tuning (03_HT): Optimizaci칩n de hiperpar치metros con Bayesian Optimization
# ============================================================================

# ============================================================================
# SECCI칍N 1: ENVIRONMENT - Configuraci칩n del entorno de trabajo
# ============================================================================
environment:
  # Directorio base del proyecto (donde est치n las carpetas codigo_base, dataset, exp)
  base_dir: "~/Materias/health_economics_challenge"

  # Directorio donde est치n los datos de entrada
  data_dir: "./dataset"

# ============================================================================
# SECCI칍N 2: EXPERIMENT - Configuraci칩n general del experimento
# ============================================================================
experiment:
  # restart: Si es TRUE, borra y reinicia el experimento. FALSE contin칰a desde donde qued칩
  restart: FALSE

  # Directorio donde se guardar치n todos los resultados del experimento
  exp_dir: "./exp"

  # experiment_label: Etiqueta principal del experimento (relacionada con la variable objetivo)
  experiment_label: "hf3_max"

  # experiment_code: Nombre descriptivo de este experimento en particular
  experiment_code: "max_minimo"

  # NOTA: Los resultados se guardan en: exp/hf3_max_minimo/hf3_max_minimo_f1/
  #       donde "f1" indica que estamos prediciendo 1 a침o hacia adelante (lead=1)

# ============================================================================
# SECCI칍N 3: FEATURE ENGINEERING - Creaci칩n de variables predictoras
# ============================================================================
feature_engineering:

  # --- Archivos de entrada/salida ---
  files:
    input:
      # Dataset de entrada con datos hist칩ricos de salud y econom칤a
      dentrada: ["dataset_desafio.csv"]

    # Script de R que ejecuta el feature engineering
    fe_script: "01_FE_health.R"

  # --- Constantes (no modificar a menos que sepas lo que haces) ---
  const:
    # origen_clase: Variable del dataset que contiene la variable objetivo
    # hf3_ppp_pc = Gasto de bolsillo per capita en paridad de poder adquisitivo (PPP)
    origen_clase: "hf3_ppp_pc"

    # clase: Nombre de la variable objetivo despu칠s de transformarla
    clase: "clase"

    # orden_lead: Cu치ntos a침os hacia el futuro queremos predecir
    # 1 = Predecir el a침o siguiente (desde 2021 predecir 2022)
    orden_lead: 1

    # presente: 칔ltimo a침o disponible en el dataset (a침o m치s reciente con datos)
    presente: 2021

    # canaritos_year_start: A침o desde el cual comenzar el an치lisis
    # (a침os anteriores se descartan)
    canaritos_year_start: 2000

    # canaritos_year_end: A침o hasta el cual usar para entrenar canaritos
    # (t칤picamente el a침o antes del test)
    canaritos_year_end: 2019

    # canaritos_year_valid: A침o usado para validaci칩n en el proceso de canaritos
    canaritos_year_valid: 2019

    # campos_sort: Orden para organizar los datos (Pa칤s, luego A침o)
    campos_sort: ["Country Code", "year"]

    # campos_rsort: Orden inverso (A침o, luego Pa칤s)
    campos_rsort: ["year", "Country Code"]

    # campos_fijos: Columnas que SIEMPRE se mantienen en el dataset
    # Estas son las variables de identificaci칩n y la variable objetivo
    campos_fijos: ["Country Name", "Country Code", "year", "region", "income", "hf3_ppp_pc","clase"]

  # --- Par치metros de Feature Engineering ---
  param:

    # dummiesNA: Si es TRUE, crea variables dummy (0/1) indicando si hab칤a un valor faltante
    # Esto es 칰til porque "falta de dato" puede ser informaci칩n valiosa
    # Ejemplo: si falta "gasto_salud", se crea "gasto_salud_NA" = 1
    dummiesNA: TRUE

    # corregir: Correcci칩n de drift (cambios en la distribuci칩n de datos a lo largo del tiempo)
    # "NO" = No aplicar correcci칩n de drift
    corregir: "NO"

    # variablesmanuales: Si es TRUE, agrega variables creadas manualmente en el script FE
    # (por ejemplo: ratios, interacciones, transformaciones espec칤ficas de salud)
    variablesmanuales: TRUE

    # acumulavars: Si es TRUE, acumula valores de variables a lo largo del tiempo
    # FALSE = No crear variables acumuladas
    acumulavars: FALSE

    # -------------------------------------------------------------------
    # LAGS: Variables retrasadas (valores de a침os anteriores)
    # -------------------------------------------------------------------
    # Los lags son cruciales en series de tiempo porque el pasado ayuda a predecir el futuro
    # Ejemplo: El gasto en salud del a침o pasado (lag_1) es un buen predictor del gasto de este a침o

    lags:
      # correr: Array que indica qu칠 lags crear
      # [TRUE, FALSE, FALSE...] = Solo crear lag de 1 a침o
      # Si pones [TRUE, TRUE, FALSE...] crear칤as lags de 1 y 2 a침os
      correr: [TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE]

      # lag: Define los per칤odos de retraso (1=a침o anterior, 2=hace 2 a침os, etc.)
      lag: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]

      # delta: Si es TRUE, crea la DIFERENCIA entre el valor actual y el lag
      # Ejemplo: delta_1_GDP = GDP_2021 - GDP_2020 (crecimiento del PIB)
      delta: [TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE]

      # canaritos: Umbral de importancia. 0.0 = No filtrar variables por importancia
      # Si pones 0.2, solo se mantienen variables con importancia > 0.2
      canaritos: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

    # -------------------------------------------------------------------
    # TENDENCIAS Y ESTAD칈STICAS M칍VILES (rolling windows)
    # -------------------------------------------------------------------
    # Calcula estad칤sticas sobre ventanas de tiempo (ej: promedio de los 칰ltimos 2 a침os)
    # Esto captura tendencias y patrones temporales

    tendenciaYmuchomas:
      # correr: Qu칠 ventanas activar. [TRUE, FALSE...] = Solo ventana de 2 a침os
      correr: [TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE]

      # ventana: Tama침o de cada ventana en a침os
      ventana: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20]

      # tendencia: Si es TRUE, calcula la PENDIENTE (crecimiento/decrecimiento) en la ventana
      # Ejemplo: 쮼l PIB est치 creciendo o cayendo en los 칰ltimos 2 a침os?
      tendencia: [TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE]

      # minimo: Si es TRUE, calcula el VALOR M칈NIMO en la ventana
      # Ejemplo: PIB m치s bajo en los 칰ltimos 2 a침os
      minimo: [TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE]

      # maximo: Si es TRUE, calcula el VALOR M츼XIMO en la ventana
      # Ejemplo: Gasto en salud m치s alto en los 칰ltimos 2 a침os
      maximo: [TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE]

      # promedio: Si es TRUE, calcula el PROMEDIO en la ventana
      # Ejemplo: Expectativa de vida promedio en los 칰ltimos 2 a침os
      promedio: [TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE]

      # ratioavg: Si es TRUE, calcula VALOR_ACTUAL / PROMEDIO_VENTANA
      # Ejemplo: Si ratio > 1, el valor actual est치 por encima del promedio hist칩rico
      ratioavg: [TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE]

      # ratiomax: Si es TRUE, calcula VALOR_ACTUAL / M츼XIMO_VENTANA
      # Ejemplo: 쯈u칠 tan cerca est치 el valor actual del m치ximo hist칩rico?
      ratiomax: [TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE]

      # canaritos: Filtro de importancia (0.0 = mantener todas)
      canaritos: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

    # -------------------------------------------------------------------
    # VARIABLES ESPEC칈FICAS DE HEALTH ECONOMICS
    # -------------------------------------------------------------------

    # health_ratios: Crea ratios de eficiencia del sistema de salud
    # Ejemplo: gasto_salud / PIB, expectativa_vida / gasto_salud, etc.
    health_ratios: TRUE

    # qaly_calculation: Calcula QALYs aproximados (Quality-Adjusted Life Years)
    # M칠trica que combina expectativa de vida y calidad de vida
    qaly_calculation: TRUE

    # crisis_dummies: Crea variables dummy para crisis econ칩micas
    # Ejemplo: dummy_crisis_2008, dummy_covid_2020
    crisis_dummies: TRUE

    # rankeador: Crea rankings relativos por regi칩n/nivel de ingreso
    # Ejemplo: Posici칩n del pa칤s en gasto de salud dentro de su regi칩n
    rankeador: TRUE

    # canaritos_final: Filtro final de importancia despu칠s de crear todas las variables
    # 0.0 = Mantener todas las variables creadas
    # 0.1 = Solo mantener variables con importancia > 0.1
    canaritos_final: 0.0

# ============================================================================
# SECCI칍N 4: TRAINING STRATEGY - Particionamiento de datos
# ============================================================================
# Divide el dataset en diferentes conjuntos para entrenar y evaluar el modelo
# Esto es crucial para evitar OVERFITTING (que el modelo memorice en lugar de aprender)
training_strategy:

  # --- Constantes ---
  const:
    # secciones: Los 5 conjuntos de datos que se crear치n
    # - present: Datos del 2021 (sin variable objetivo) para hacer predicciones de 2022
    # - train: Datos antiguos para entrenar el modelo
    # - validate: Datos para ajustar hiperpar치metros
    # - test: Datos para evaluar el modelo final (simula datos nunca vistos)
    # - train_final: train + validate para entrenar el modelo final con m치s datos
    secciones: ["present", "train", "validate", "test", "train_final"]

    # clase: Nombre de la variable objetivo
    clase: "clase"

    # periodo: Campo que indica el tiempo (en este caso, a침o)
    periodo: "year"

    # campos_sort: Orden para organizar los datos
    campos_sort: ["Country Code", "year"]

  # --- Par치metros de particionamiento ---
  param:
    # semilla: Semilla para reproducibilidad (siempre genera las mismas particiones)
    semilla: 123456

    # NOTA IMPORTANTE: Los campos 'periodos' y 'rango$hasta' se calculan AUTOM츼TICAMENTE
    # en el script 02_TS_health.R bas치ndose en el a침o 'presente' (2021) y 'orden_lead' (1)
    #
    # El script calcula autom치ticamente:
    #   - test: a침o 2020 (칰ltimo a침o con clase disponible = 2021 - 1)
    #   - validate: a침o 2019 (a침o anterior al test)
    #   - train: desde 2000 hasta 2018 (2 a침os antes del test)
    #   - train_final: desde 2000 hasta 2020 (incluye train + validate + test)
    #   - present: a침o 2021 (sin clase, para predecir 2022)

    # --- TRAIN: Conjunto de entrenamiento ---
    train:
      rango:
        # desde: A침o inicial para entrenamiento (칔NICO campo que debes configurar)
        desde: 2000
        # hasta: Se calcula autom치ticamente (2018 en este caso)

      # excluir: Lista de a침os espec칤ficos a excluir del training
      # Ejemplo: [2008, 2009] excluir칤a los a침os de la crisis financiera
      excluir: [2020, 2021]

      # undersampling: Reducir la cantidad de ciertos registros para balancear clases
      # En regresi칩n t칤picamente se deja vac칤o. 칔til en clasificaci칩n desbalanceada
      undersampling: []

    # --- VALIDATE: Conjunto de validaci칩n ---
    validate:
      # periodos y rango se calculan autom치ticamente (a침o 2019 en este caso)
      excluir: []
      undersampling: []

    # --- TEST: Conjunto de prueba ---
    test:
      # periodos y rango se calculan autom치ticamente (a침o 2020 en este caso)
      excluir: []
      undersampling: []

    # --- TRAIN_FINAL: Entrenamiento final con m치s datos ---
    train_final:
      rango:
        # desde: A침o inicial (mismo que train)
        desde: 2000
        # hasta: Se calcula autom치ticamente (2020 en este caso)
      excluir: []
      undersampling: []

  # --- Archivos de salida ---
  files:
    output:
      # present_data: Datos de 2021 sin clase (para predecir 2022)
      present_data: "TS_present_data.csv.gz"

      # train_strategy: Datos de train + validate + test (con particiones marcadas)
      train_strategy: "TS_train_strategy.csv.gz"

      # train_final: Datos para entrenar el modelo final (train + validate + test)
      train_final: "TS_train_final.csv.gz"

      # control: Archivo de texto con resumen de las particiones (revisar siempre!)
      control: "control.txt"

    # Script que ejecuta el particionamiento
    ts_script: "02_TS_health.R"

# ============================================================================
# SECCI칍N 5: HYPERPARAMETER TUNING - Optimizaci칩n de hiperpar치metros
# ============================================================================
# Usa Optimizaci칩n Bayesiana para encontrar los mejores hiperpar치metros de LightGBM
# que minimicen el error de predicci칩n (RMSE)
hyperparameter_tuning:

  # --- Configuraci칩n general ---
  param:
    # algoritmo: Algoritmo de Machine Learning a usar
    algoritmo: "lightgbm"

    # semilla: Para reproducibilidad de resultados
    semilla: 123456

    # crossvalidation: Si usar validaci칩n cruzada (divide train en K folds)
    # FALSE = Usar conjunto de validaci칩n separado (m치s r치pido)
    crossvalidation: FALSE

    # validate: Si TRUE, usa el conjunto 'validate' para evaluar durante el tuning
    validate: TRUE

    # -----------------------------------------------------------------------
    # HIPERPAR츼METROS DE LIGHTGBM
    # -----------------------------------------------------------------------
    # FORMATO: [valor_m칤nimo, valor_m치ximo] = Rango a explorar por Bayesian Optimization
    #          valor_fijo = No se optimiza, se usa ese valor siempre

    lightgbm:

      # --- Hiperpar치metros de APRENDIZAJE ---

      # learning_rate: Tasa de aprendizaje (cu치nto aprende en cada iteraci칩n)
      # Valores bajos (~0.01): Aprendizaje lento pero preciso (requiere m치s iteraciones)
      # Valores altos (~0.2): Aprendizaje r치pido pero puede ser inestable
      # RANGO A EXPLORAR: [0.01, 0.2]
      learning_rate: [0.01, 0.2]

      # feature_fraction: Fracci칩n de variables a usar en cada 치rbol
      # 0.3 = Usa solo 30% de las variables (m치s r치pido, previene overfitting)
      # 0.9 = Usa 90% de las variables (m치s informaci칩n, pero m치s lento)
      # RANGO A EXPLORAR: [0.3, 0.9]
      feature_fraction: [0.3, 0.9]

      # num_leaves: N칰mero de hojas en cada 치rbol
      # Valores bajos (~10): Modelos simples (previenen overfitting)
      # Valores altos (~1500): Modelos complejos (pueden overfittear)
      # El tercer valor (1) indica que es un ENTERO
      # RANGO A EXPLORAR: [10, 1500] (enteros)
      num_leaves: [10, 1500, 1]

      # min_data_in_leaf: M칤nimo de registros necesarios en cada hoja
      # Valores bajos (~5): Hojas con pocos datos (m치s detalle, riesgo de overfitting)
      # Valores altos (~100): Hojas con muchos datos (m치s generalizaci칩n)
      # RANGO A EXPLORAR: [5, 100] (enteros)
      min_data_in_leaf: [5, 250, 1]

      # --- Hiperpar치metros de REGULARIZACI칍N ---
      # Ayudan a prevenir overfitting penalizando modelos complejos

      # lambda_l1: Regularizaci칩n L1 (Lasso)
      # Tiende a eliminar variables poco importantes (coeficientes a 0)
      # 0 = Sin regularizaci칩n L1
      # 5 = Regularizaci칩n fuerte
      # RANGO A EXPLORAR: [0, 5]
      lambda_l1: [0, 5]

      # lambda_l2: Regularizaci칩n L2 (Ridge)
      # Reduce el valor de los coeficientes sin eliminarlos
      # 0 = Sin regularizaci칩n L2
      # 10 = Regularizaci칩n fuerte
      # RANGO A EXPLORAR: [0, 10]
      lambda_l2: [0, 10]

      # min_gain_to_split: Ganancia m칤nima para dividir un nodo
      # 0 = Permite cualquier divisi칩n (m치s flexible)
      # Valores mayores previenen divisiones poco 칰tiles
      # VALOR FIJO: 0
      min_gain_to_split: 0

      # --- Hiperpar치metros de BAGGING ---
      # Bagging = Entrenar cada 치rbol con una muestra aleatoria de datos

      # bagging_fraction: Fracci칩n de datos a usar para entrenar cada 치rbol
      # 0.6 = Usa 60% de los datos (m치s diversidad entre 치rboles)
      # 1.0 = Usa 100% de los datos (menos diversidad)
      # RANGO A EXPLORAR: [0.6, 1.0]
      bagging_fraction: [0.6, 1.0]

      # --- Hiperpar치metros de ESTRUCTURA DE 츼RBOLES ---

      # max_depth: Profundidad m치xima de cada 치rbol
      # -1 = Sin l칤mite (el 치rbol crece lo necesario)
      # Valores positivos limitan la profundidad (previenen overfitting)
      # VALOR FIJO: -1 (sin l칤mite, num_leaves ya controla complejidad)
      max_depth: -1

      # max_bin: M치ximo de bins para discretizar variables continuas
      # 255 = Valor est치ndar (buen balance entre precisi칩n y velocidad)
      # VALOR FIJO: 255
      max_bin: 255

      # --- Hiperpar치metros de REPRODUCIBILIDAD ---

      # seed: Semilla para n칰meros aleatorios dentro de LightGBM
      seed: 999983

      # extra_trees: Si TRUE, usa Extremely Randomized Trees
      # FALSE = Usa la estrategia est치ndar de divisi칩n (mejor para la mayor칤a de casos)
      extra_trees: FALSE

      # --- Hiperpar치metros de EVALUACI칍N ---

      # metric: M칠trica de error a minimizar
      # "rmse" = Root Mean Squared Error (ra칤z del error cuadr치tico medio)
      # Penaliza fuertemente errores grandes
      metric: "rmse"

      # first_metric_only: Si TRUE, solo usa la primera m칠trica para early stopping
      first_metric_only: TRUE

      # objective: Tipo de problema a resolver
      # "regression" = Problema de regresi칩n (predecir valor continuo)
      objective: "regression"

      # --- Hiperpar치metros T칄CNICOS ---

      # boost_from_average: Si TRUE, inicializa desde el promedio de la variable objetivo
      # TRUE = Convergencia m치s r치pida (recomendado)
      boost_from_average: TRUE

      # force_row_wise: Si TRUE, fuerza procesamiento por filas
      # TRUE = M치s r치pido para datasets con muchas variables
      force_row_wise: TRUE

      # feature_pre_filter: Si TRUE, pre-filtra variables antes de entrenar
      # FALSE = Usa todas las variables (recomendado)
      feature_pre_filter: FALSE

      # boosting: Tipo de boosting
      # "gbdt" = Gradient Boosting Decision Tree (est치ndar, m치s efectivo)
      # Alternativas: "dart" (con dropout), "goss" (muestreo de gradientes)
      boosting: "gbdt"

      # num_threads: N칰mero de threads para paralelizaci칩n
      # 0 = Usa todos los cores disponibles
      num_threads: 0

      # verbosity y verbose: Nivel de mensajes en consola
      # -100 = Silencioso (no muestra nada)
      verbosity: -100
      verbose: -100

    # -----------------------------------------------------------------------
    # CONFIGURACI칍N DE BAYESIAN OPTIMIZATION (BO)
    # -----------------------------------------------------------------------
    # La BO busca inteligentemente en el espacio de hiperpar치metros
    # para encontrar la mejor combinaci칩n en pocas iteraciones

    BO:
      # iterations: N칰mero de combinaciones de hiperpar치metros a probar
      # 100 iteraciones es un buen balance entre tiempo y calidad de resultados
      # Menos iteraciones = M치s r치pido pero peores resultados
      # M치s iteraciones = M치s lento pero mejores resultados
      iterations: 100

      # noisy: Si TRUE, asume que la evaluaci칩n tiene ruido (variabilidad)
      # TRUE = Recomendado (hay variabilidad en el proceso de entrenamiento)
      noisy: TRUE

      # minimize: Si TRUE, busca MINIMIZAR la m칠trica (RMSE en este caso)
      # TRUE = Queremos el menor error posible
      minimize: TRUE

      # has.simple.signature: Configuraci칩n t칠cnica de mlrMBO
      # FALSE = Pasar par치metros como lista (formato usado en este pipeline)
      has.simple.signature: FALSE

      # save.on.disk.at.time: Cada cu치ntos segundos guardar el progreso
      # 600 segundos = 10 minutos
      # Permite retomar si se interrumpe el proceso
      save.on.disk.at.time: 600

  # --- Archivos de entrada/salida ---
  files:
    input:
      # dentrada: Dataset con las particiones train/validate/test
      dentrada: "TS_train_strategy.csv.gz"

    output:
      # BOlog: Log con todas las iteraciones y sus resultados (revisar siempre!)
      BOlog: "BO_log.txt"

      # BObin: Archivo binario para retomar la optimizaci칩n si se interrumpe
      BObin: "BO_bin.Rdata"

      # tb_importancia: Tabla con la importancia de cada variable
      # Ayuda a entender qu칠 variables son m치s relevantes para la predicci칩n
      tb_importancia: "tb_importancia.txt"

      # importancia: Prefijo para archivos de importancia intermedios
      # Se guarda un archivo cada vez que se encuentra un mejor modelo
      importancia: "impo_"

    # Script que ejecuta la optimizaci칩n de hiperpar치metros
    ht_script: "03_HT_health_v2.R"

  # --- Constantes ---
  const:
    # campo_clase: Nombre de la variable objetivo en el dataset
    campo_clase: "clase"

    # campo_periodo: Nombre de la variable temporal (a침o)
    campo_periodo: "year"

# ============================================================================
# FIN DEL ARCHIVO DE CONFIGURACI칍N
# ============================================================================
#
# RECOMENDACIONES PARA ALUMNOS DE LA UNO:
#
# 1. EXPERIMENTACI칍N:
#    - Copia este archivo con otro nombre (ej: CONFIG_experimento1.yml)
#    - Modifica UN par치metro a la vez para ver su efecto
#    - Documenta tus cambios y resultados
#
# 2. PAR츼METROS CLAVE PARA MODIFICAR:
#    - feature_engineering -> lags -> correr: Prueba con m치s lags
#    - feature_engineering -> tendenciaYmuchomas -> correr: Prueba m치s ventanas
#    - hyperparameter_tuning -> lightgbm: Ajusta los rangos de b칰squeda
#    - hyperparameter_tuning -> BO -> iterations: M치s iteraciones = mejores resultados
#
# 3. ARCHIVOS A REVISAR DESPU칄S DE CORRER:
#    - exp/hf3_max_minimo/hf3_max_minimo_f1/02_TS/control.txt (verifica particiones)
#    - exp/hf3_max_minimo/hf3_max_minimo_f1/03_HT/BO_log.txt (resultados de BO)
#    - exp/hf3_max_minimo/hf3_max_minimo_f1/03_HT/tb_importancia.txt (variables importantes)
#
# 4. C칍MO EJECUTAR:
#    - Modifica la l칤nea 14 de codigo_base/0_HEALTH_EXE.R con el nombre de tu YAML
#    - Ejecuta: source("codigo_base/0_HEALTH_EXE.R")
#    - Espera a que termine (puede tomar horas seg칰n iterations)
#
# 5. M칄TRICAS DE 칄XITO:
#    - RMSE m치s bajo en el conjunto de TEST
#    - Predicciones coherentes con la realidad de 2022
#    - Modelo que generaliza bien (no overfitting)
#
# 춰칄xitos en el desaf칤o! 游꿉游늵
# ============================================================================
