# ExplicaciÃ³n del Cambio: Hyperparameter Tuning v1 â†’ v2

**Universidad Nacional del Oeste (UNO)**  
**Curso:** Aplicaciones en Ciencia de Datos  
**Tema:** CorrecciÃ³n del data leakage en optimizaciÃ³n bayesiana

---

## ğŸ¯ El Problema

En la versiÃ³n 1 del cÃ³digo, cometÃ­amos un error sutil pero importante: usÃ¡bamos el **mismo conjunto de datos (VALIDATE)** para dos propÃ³sitos diferentes:

1. **Early stopping**: decidir cuÃ¡ndo parar de entrenar
2. **Evaluar hiperparÃ¡metros**: reportar el RMSE a la optimizaciÃ³n bayesiana

Esto genera un sesgo optimista en la evaluaciÃ³n de hiperparÃ¡metros.

---

## ğŸ“ CÃ³digo v1 (PROBLEMÃTICO)

```r
EstimarGanancia_lightgbm <- function(x) {
  
  # HiperparÃ¡metros a optimizar
  param_completo <- list(
    boosting = "gbdt",
    objective = "regression",
    metric = "rmse",
    learning_rate = x$learning_rate,
    num_leaves = x$num_leaves,
    min_data_in_leaf = x$min_data_in_leaf,
    feature_fraction = x$feature_fraction,
    bagging_fraction = x$bagging_fraction,
    bagging_freq = x$bagging_freq,
    lambda_l1 = x$lambda_l1,
    lambda_l2 = x$lambda_l2,
    max_depth = x$max_depth,
    verbose = -1
  )
  
  # Entrenar modelo
  modelo <- lgb.train(
    data = dtrain,
    valids = list(valid = dvalidate),
    params = param_completo,
    nrounds = 2000,
    early_stopping_rounds = 50,
    verbose = -1
  )
  
  # âš ï¸ PROBLEMA: Tomamos el RMSE del mismo conjunto usado para early stopping
  best_iter <- modelo$best_iter
  rmse_validate <- modelo$record_evals$valid$rmse[[best_iter]]
  
  return(list(Score = -rmse_validate, Pred = 0))
}
```

### Â¿Por quÃ© es problemÃ¡tico?

| Paso | QuÃ© usa | Conjunto |
|------|---------|----------|
| Early stopping | `valids = list(valid = dvalidate)` | VALIDATE (2019) |
| RMSE para BO | `modelo$record_evals$valid$rmse` | VALIDATE (2019) âš ï¸ |

El modelo ya "vio" VALIDATE durante el entrenamiento para decidir cuÃ¡ndo parar. DespuÃ©s usamos **ese mismo RMSE** para guiar la optimizaciÃ³n bayesiana. Es como si el Ã¡rbitro del entrenamiento tambiÃ©n fuera el juez de la competencia final.

---

## âœ… CÃ³digo v2 (CORRECTO)

```r
EstimarGanancia_lightgbm <- function(x) {
  
  # HiperparÃ¡metros a optimizar
  param_completo <- list(
    boosting = "gbdt",
    objective = "regression",
    metric = "rmse",
    learning_rate = x$learning_rate,
    num_leaves = x$num_leaves,
    min_data_in_leaf = x$min_data_in_leaf,
    feature_fraction = x$feature_fraction,
    bagging_fraction = x$bagging_fraction,
    bagging_freq = x$bagging_freq,
    lambda_l1 = x$lambda_l1,
    lambda_l2 = x$lambda_l2,
    max_depth = x$max_depth,
    verbose = -1
  )
  
  # Entrenar modelo (early stopping usa VALIDATE)
  modelo <- lgb.train(
    data = dtrain,
    valids = list(valid = dvalidate),
    params = param_completo,
    nrounds = 2000,
    early_stopping_rounds = 50,
    verbose = -1
  )
  
  # âœ… CORRECTO: Evaluamos en TEST, un conjunto completamente independiente
  predicciones <- predict(modelo, dtest_matrix)
  rmse_test <- sqrt(mean((predicciones - datos_test$target)^2))
  
  return(list(Score = -rmse_test, Pred = 0))
}
```

### Â¿Por quÃ© es correcto?

| Paso | QuÃ© usa | Conjunto |
|------|---------|----------|
| Early stopping | `valids = list(valid = dvalidate)` | VALIDATE (2019) |
| RMSE para BO | `predict(modelo, dtest_matrix)` | TEST (2020) âœ… |

Ahora cada conjunto tiene **un Ãºnico propÃ³sito**:
- **VALIDATE** â†’ Solo para early stopping (cuÃ¡ndo parar de agregar Ã¡rboles)
- **TEST** â†’ EvaluaciÃ³n "fresca" que guÃ­a la optimizaciÃ³n bayesiana

---

## ğŸ” Diferencia Lado a Lado

```r
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# v1: PROBLEMÃTICO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  best_iter <- modelo$best_iter
  rmse_validate <- modelo$record_evals$valid$rmse[[best_iter]]
  #                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  #                       Usa el RMSE que el modelo ya vio
  #                       durante el proceso de early stopping
  
  return(list(Score = -rmse_validate, Pred = 0))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# v2: CORRECTO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  predicciones <- predict(modelo, dtest_matrix)
  #                               ^^^^^^^^^^^^
  #                               Datos que el modelo NUNCA vio
  #                               durante ninguna parte del entrenamiento
  
  rmse_test <- sqrt(mean((predicciones - datos_test$target)^2))
  
  return(list(Score = -rmse_test, Pred = 0))
```

---

## ğŸ“Š Impacto Esperado

| Aspecto | v1 | v2 |
|---------|----|----|
| RMSE reportado durante BO | Optimista (sesgado) | Realista |
| Riesgo de overfitting a hiperparÃ¡metros | Alto | Bajo |
| GeneralizaciÃ³n a datos futuros | Peor | Mejor |
| Honestidad de la evaluaciÃ³n | âŒ | âœ… |

---

## ğŸ§  Concepto Clave: Early Stopping

**Early stopping** es una tÃ©cnica de regularizaciÃ³n que detiene el entrenamiento cuando el modelo deja de mejorar en un conjunto de validaciÃ³n.

```
IteraciÃ³n   RMSE (train)   RMSE (validate)
â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   100         0.45            0.52        â† mejorando
   200         0.35            0.48        â† mejorando  
   300         0.25            0.45        â† MEJOR PUNTO â˜…
   400         0.18            0.46        â† validate empeora...
   450         0.15            0.47        â† sigue empeorando...
   500 â›”      0.12            0.48        â† STOP! (50 rounds sin mejorar)
```

El modelo se queda con los parÃ¡metros de la **iteraciÃ³n 300** (mejor en validate).

### Â¿Por quÃ© necesitamos early stopping?

Sin early stopping, el modelo seguirÃ­a entrenando hasta las 2000 rondas, memorizando cada vez mÃ¡s el conjunto de entrenamiento y perdiendo capacidad de generalizaciÃ³n.

---

## ğŸ“ Archivos Relacionados

- `comparacion_ht_v1_v2.dot` â†’ Diagrama visual del cambio
- `comparacion_ht_v1_v2.pdf` â†’ VersiÃ³n compilada del diagrama

---

## ğŸ“ Moraleja PedagÃ³gica

> **"Cada conjunto de datos debe tener UN Ãºnico propÃ³sito."**

En Machine Learning riguroso:
- **TRAIN** â†’ Entrenar el modelo
- **VALIDATE** â†’ Decisiones durante el entrenamiento (early stopping, selecciÃ³n de arquitectura)
- **TEST** â†’ EvaluaciÃ³n final, completamente independiente

Cuando un conjunto hace "doble trabajo", perdemos la honestidad de nuestra evaluaciÃ³n.
